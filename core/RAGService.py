class RAGService():
	def __init__(self,vector_service,context_service,llm):
		self.vector_service = vector_service
		self.context_service = context_service
		self.llm = llm
		
	def answer(self,query:str):
		"""RAG main function generates response by orchestating vector_service and context_service and main llm to generate response
		Args:
			query: str query from the user
		Returns:
			response: str (generated by llm)
		"""
		
		rewritten_query = self.context_service.build_query(query)
		print(f"User query is Rewritten as :{rewritten_query}")
		retrieved_chunks = self.vector_service.search(rewritten_query,k=3)   """k is set to 3, retrieved_chunks = {metadata: {......}
															similarity: float}  content can be utilised retrieved_chunks["metadat"]["content"]"""
		for chunk in retrieved_chunks:
    		print(f"retrieved chunk:\n\n{chunk['metadata']['content']}\n")
    		
    		num_chunks = len(retrieved_chunks)
    		
    		# reranking the chunks
    		reranked_chunks = self.context_service.rerank(rewritten_query,
    					retrieved_chunks,
    					num_chunks)
		context = self.context_service.chunk_compressor(reranked_chunks,rewritten_query)  # Now we have the context
		
		# Response generation using main llm
		prompt = f"""
		Use this context below to answer the User query:
		Context:
		{context}
		
		Question:
		{rewritten_query}
		"""
		
		return self.llm.invoke(prompt)
												
						   										
